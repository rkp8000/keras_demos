{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 64s 0us/step\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.8429 - acc: 0.3213 - val_loss: 1.5259 - val_acc: 0.4506\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.5585 - acc: 0.4291 - val_loss: 1.3569 - val_acc: 0.5185\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.4347 - acc: 0.4832 - val_loss: 1.2597 - val_acc: 0.5497\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 1.3486 - acc: 0.5171 - val_loss: 1.1996 - val_acc: 0.5798\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 1.2759 - acc: 0.5448 - val_loss: 1.0989 - val_acc: 0.6148\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.2086 - acc: 0.5710 - val_loss: 1.0700 - val_acc: 0.6292\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1605 - acc: 0.5895 - val_loss: 1.0447 - val_acc: 0.6345\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1184 - acc: 0.6026 - val_loss: 0.9777 - val_acc: 0.6527\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.0816 - acc: 0.6170 - val_loss: 0.9552 - val_acc: 0.6653\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.0415 - acc: 0.6315 - val_loss: 0.8940 - val_acc: 0.6890\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.0153 - acc: 0.6393 - val_loss: 0.8971 - val_acc: 0.6850\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.9955 - acc: 0.6518 - val_loss: 0.8924 - val_acc: 0.6877\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.9663 - acc: 0.6597 - val_loss: 0.8330 - val_acc: 0.7078\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.9489 - acc: 0.6676 - val_loss: 0.8346 - val_acc: 0.7151\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9280 - acc: 0.6739 - val_loss: 0.7925 - val_acc: 0.7235\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.9140 - acc: 0.6797 - val_loss: 0.7994 - val_acc: 0.7222\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.8997 - acc: 0.6848 - val_loss: 0.7820 - val_acc: 0.7299\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.8914 - acc: 0.6887 - val_loss: 0.7857 - val_acc: 0.7261\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8794 - acc: 0.6927 - val_loss: 0.7755 - val_acc: 0.7268\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.8677 - acc: 0.6977 - val_loss: 0.7712 - val_acc: 0.7326\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.8589 - acc: 0.7006 - val_loss: 0.7363 - val_acc: 0.7455\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8506 - acc: 0.7057 - val_loss: 0.7238 - val_acc: 0.7480\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.8427 - acc: 0.7069 - val_loss: 0.7146 - val_acc: 0.7570\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8409 - acc: 0.7087 - val_loss: 0.7050 - val_acc: 0.7560\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.8296 - acc: 0.7114 - val_loss: 0.7181 - val_acc: 0.7568\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.8218 - acc: 0.7187 - val_loss: 0.7257 - val_acc: 0.7492\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.8223 - acc: 0.7184 - val_loss: 0.7015 - val_acc: 0.7596\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.8141 - acc: 0.7198 - val_loss: 0.6880 - val_acc: 0.7635\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.8045 - acc: 0.7235 - val_loss: 0.6767 - val_acc: 0.7663\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.8016 - acc: 0.7247 - val_loss: 0.6974 - val_acc: 0.7649\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.7989 - acc: 0.7258 - val_loss: 0.6780 - val_acc: 0.7685\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.7996 - acc: 0.7256 - val_loss: 0.6760 - val_acc: 0.7661\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.7945 - acc: 0.7275 - val_loss: 0.6905 - val_acc: 0.7757\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7840 - acc: 0.7318 - val_loss: 0.6718 - val_acc: 0.7741\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7907 - acc: 0.7319 - val_loss: 0.6759 - val_acc: 0.7793\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.7831 - acc: 0.7320 - val_loss: 0.6784 - val_acc: 0.7742\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.7862 - acc: 0.7330 - val_loss: 0.6670 - val_acc: 0.7754\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.7799 - acc: 0.7326 - val_loss: 0.6545 - val_acc: 0.7770\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.7805 - acc: 0.7351 - val_loss: 0.6612 - val_acc: 0.7761\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7791 - acc: 0.7361 - val_loss: 0.7545 - val_acc: 0.7673\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7715 - acc: 0.7370 - val_loss: 0.6727 - val_acc: 0.7750\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.7726 - acc: 0.7372 - val_loss: 0.6924 - val_acc: 0.7696\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.7698 - acc: 0.7381 - val_loss: 0.6646 - val_acc: 0.7754\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7745 - acc: 0.7374 - val_loss: 0.6644 - val_acc: 0.7767\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7712 - acc: 0.7390 - val_loss: 0.6584 - val_acc: 0.7836\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.7689 - acc: 0.7403 - val_loss: 0.6792 - val_acc: 0.7851\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7668 - acc: 0.7402 - val_loss: 0.6618 - val_acc: 0.7794\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7617 - acc: 0.7402 - val_loss: 0.6632 - val_acc: 0.7885\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7621 - acc: 0.7430 - val_loss: 0.6636 - val_acc: 0.7755\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.7592 - acc: 0.7419 - val_loss: 0.6691 - val_acc: 0.7781\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.7566 - acc: 0.7452 - val_loss: 0.6357 - val_acc: 0.7826\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7603 - acc: 0.7450 - val_loss: 0.6161 - val_acc: 0.7934\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7574 - acc: 0.7439 - val_loss: 0.6289 - val_acc: 0.7901\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7563 - acc: 0.7460 - val_loss: 0.6370 - val_acc: 0.7869\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7565 - acc: 0.7448 - val_loss: 0.6286 - val_acc: 0.7930\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7537 - acc: 0.7451 - val_loss: 0.6350 - val_acc: 0.7890\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.7476 - acc: 0.7470 - val_loss: 0.6547 - val_acc: 0.7849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7556 - acc: 0.7457 - val_loss: 0.6345 - val_acc: 0.7851\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7564 - acc: 0.7463 - val_loss: 0.6056 - val_acc: 0.7918\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7486 - acc: 0.7488 - val_loss: 0.6739 - val_acc: 0.7761\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7478 - acc: 0.7476 - val_loss: 0.6612 - val_acc: 0.7835\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7505 - acc: 0.7471 - val_loss: 0.6540 - val_acc: 0.7854\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7549 - acc: 0.7471 - val_loss: 0.6593 - val_acc: 0.7861\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.7551 - acc: 0.7465 - val_loss: 0.6475 - val_acc: 0.7778\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7557 - acc: 0.7483 - val_loss: 0.6420 - val_acc: 0.7842\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7508 - acc: 0.7496 - val_loss: 0.6799 - val_acc: 0.7852\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7507 - acc: 0.7475 - val_loss: 0.6699 - val_acc: 0.7764\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.7576 - acc: 0.7458 - val_loss: 0.6745 - val_acc: 0.7799\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7498 - acc: 0.7498 - val_loss: 0.6876 - val_acc: 0.7834\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.7568 - acc: 0.7472 - val_loss: 0.6697 - val_acc: 0.7780\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.7567 - acc: 0.7477 - val_loss: 0.7027 - val_acc: 0.7687\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7490 - acc: 0.7489 - val_loss: 0.6614 - val_acc: 0.7795\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7550 - acc: 0.7475 - val_loss: 0.6622 - val_acc: 0.7801\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7550 - acc: 0.7494 - val_loss: 0.6728 - val_acc: 0.7769\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7546 - acc: 0.7487 - val_loss: 0.6799 - val_acc: 0.7693\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7600 - acc: 0.7444 - val_loss: 0.6487 - val_acc: 0.7886\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7548 - acc: 0.7479 - val_loss: 0.6866 - val_acc: 0.7678\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7588 - acc: 0.7466 - val_loss: 0.6522 - val_acc: 0.7826\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7629 - acc: 0.7479 - val_loss: 0.6887 - val_acc: 0.7777\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7587 - acc: 0.7475 - val_loss: 0.6504 - val_acc: 0.7896\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7588 - acc: 0.7497 - val_loss: 0.7008 - val_acc: 0.7668\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7601 - acc: 0.7486 - val_loss: 0.7001 - val_acc: 0.7696\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.7588 - acc: 0.7459 - val_loss: 0.6782 - val_acc: 0.7785\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.7643 - acc: 0.7465 - val_loss: 0.6179 - val_acc: 0.7905\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7611 - acc: 0.7471 - val_loss: 0.6678 - val_acc: 0.7802\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7762 - acc: 0.7442 - val_loss: 0.6458 - val_acc: 0.7829\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7712 - acc: 0.7445 - val_loss: 0.6646 - val_acc: 0.7704\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7657 - acc: 0.7453 - val_loss: 0.7120 - val_acc: 0.7680\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.7717 - acc: 0.7449 - val_loss: 0.6409 - val_acc: 0.7949\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7735 - acc: 0.7447 - val_loss: 0.6384 - val_acc: 0.7915\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.7782 - acc: 0.7433 - val_loss: 0.6651 - val_acc: 0.7799\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.7713 - acc: 0.7445 - val_loss: 0.6639 - val_acc: 0.7744\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7781 - acc: 0.7424 - val_loss: 0.7612 - val_acc: 0.7612\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.7859 - acc: 0.7409 - val_loss: 0.6607 - val_acc: 0.7814\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7841 - acc: 0.7420 - val_loss: 0.6599 - val_acc: 0.7789\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7741 - acc: 0.7448 - val_loss: 0.6825 - val_acc: 0.7777\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.7810 - acc: 0.7411 - val_loss: 0.7057 - val_acc: 0.7802\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7898 - acc: 0.7409 - val_loss: 0.6359 - val_acc: 0.7948\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7946 - acc: 0.7378 - val_loss: 0.6772 - val_acc: 0.7749\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7927 - acc: 0.7400 - val_loss: 0.7648 - val_acc: 0.7478\n",
      "Saved trained model at /home/rkp/Projects/keras_demos/saved_models/keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 2s 163us/step\n",
      "Test loss: 0.764788369751\n",
      "Test accuracy: 0.7478\n"
     ]
    }
   ],
   "source": [
    "'''Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
